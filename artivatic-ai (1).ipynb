{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14918237,"datasetId":9545472,"databundleVersionId":15784594},{"sourceType":"datasetVersion","sourceId":14929420,"datasetId":9553266,"databundleVersionId":15796858},{"sourceType":"datasetVersion","sourceId":2230405,"datasetId":1339864,"databundleVersionId":2272174},{"sourceType":"datasetVersion","sourceId":14891880,"datasetId":9528239,"databundleVersionId":15755807},{"sourceType":"datasetVersion","sourceId":14899028,"datasetId":9532994,"databundleVersionId":15763603}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===== CORE LIBRARIES =====\nimport os\nimport json\nimport re\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# ===== OCR =====\nimport pytesseract\nfrom pytesseract import Output\n\n# ===== TORCH =====\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# ===== TRANSFORMERS =====\nfrom transformers import (\n    LayoutLMv3Processor,\n    LayoutLMv3ForTokenClassification,\n    TrainingArguments,\n    Trainer\n)\n\n# ===== METRICS =====\nfrom sklearn.metrics import classification_report\n\nprint(\"Imports loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:10:36.925006Z","iopub.execute_input":"2026-02-23T06:10:36.925319Z","iopub.status.idle":"2026-02-23T06:10:38.600678Z","shell.execute_reply.started":"2026-02-23T06:10:36.925292Z","shell.execute_reply":"2026-02-23T06:10:38.600078Z"}},"outputs":[{"name":"stdout","text":"Imports loaded successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ===== PATHS =====\n\nIMAGE_ROOT = \"/kaggle/input/datasets/yamunaaab/artivatic-train-data/OneDrive_2026-02-22/Sample data\"\nANNOTATION_PATH = \"/kaggle/input/datasets/yamunaaab/annotated-json/json.txt\"  # ðŸ”´ change this to correct path\n\n\n# ===== LOAD ANNOTATIONS =====\n\nwith open(ANNOTATION_PATH, \"r\") as f:\n    annotations_raw = json.load(f)\n\nprint(\"Total annotations:\", len(annotations_raw))\n\nannotation_keys = sorted(annotations_raw.keys())\n\n\n# ===== DEFINE FOLDER ORDER (Your Batch Order) =====\n\nfolder_order = [\n    \"Detailed hospital bill\",\n    \"Diagnostic Bills\",\n    \"Discharge Bills\",\n    \"Final Bills\",\n    \"Hospital Bills\",\n    \"IPD Bills\"\n]\n\n\n# ===== LOAD ORDERED IMAGE PATHS =====\n\nordered_image_paths = []\n\nfor folder_name in folder_order:\n    folder_path = os.path.join(IMAGE_ROOT, folder_name)\n    \n    files = [\n        os.path.join(folder_path, f)\n        for f in os.listdir(folder_path)\n        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n    ]\n    \n    files = sorted(files)  # ensure consistent order\n    \n    # Since you annotated 5 per batch\n    ordered_image_paths.extend(files[:5])\n\nprint(\"Total ordered images:\", len(ordered_image_paths))\nprint(\"First 3 ordered images:\", ordered_image_paths[:3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:10:42.501238Z","iopub.execute_input":"2026-02-23T06:10:42.501797Z","iopub.status.idle":"2026-02-23T06:10:42.566559Z","shell.execute_reply.started":"2026-02-23T06:10:42.501764Z","shell.execute_reply":"2026-02-23T06:10:42.565918Z"}},"outputs":[{"name":"stdout","text":"Total annotations: 30\nTotal ordered images: 30\nFirst 3 ordered images: ['/kaggle/input/datasets/yamunaaab/artivatic-train-data/OneDrive_2026-02-22/Sample data/Detailed hospital bill/1981063-2.jpg', '/kaggle/input/datasets/yamunaaab/artivatic-train-data/OneDrive_2026-02-22/Sample data/Detailed hospital bill/2042173-10.jpg', '/kaggle/input/datasets/yamunaaab/artivatic-train-data/OneDrive_2026-02-22/Sample data/Detailed hospital bill/2042485-05.jpg']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"image_annotation_pairs = list(zip(ordered_image_paths, annotation_keys))\nprint(\"Pairs created:\", len(image_annotation_pairs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:10:47.488499Z","iopub.execute_input":"2026-02-23T06:10:47.489190Z","iopub.status.idle":"2026-02-23T06:10:47.493727Z","shell.execute_reply.started":"2026-02-23T06:10:47.489153Z","shell.execute_reply":"2026-02-23T06:10:47.493105Z"}},"outputs":[{"name":"stdout","text":"Pairs created: 30\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ===== OCR WORD + BBOX EXTRACTION =====\n\ndef get_ocr_words_boxes(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    width, height = image.size\n    \n    data = pytesseract.image_to_data(image, output_type=Output.DICT)\n    \n    words = []\n    boxes = []\n    \n    for i in range(len(data[\"text\"])):\n        word = data[\"text\"][i].strip()\n        if word == \"\":\n            continue\n        \n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n        \n        # Normalize to 0â€“1000 scale (LayoutLM requirement)\n        box = [\n            int(1000 * x / width),\n            int(1000 * y / height),\n            int(1000 * (x + w) / width),\n            int(1000 * (y + h) / height),\n        ]\n        \n        words.append(word)\n        boxes.append(box)\n    \n    return words, boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:10:55.234913Z","iopub.execute_input":"2026-02-23T06:10:55.235200Z","iopub.status.idle":"2026-02-23T06:10:55.241095Z","shell.execute_reply.started":"2026-02-23T06:10:55.235174Z","shell.execute_reply":"2026-02-23T06:10:55.240324Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test_image_path, test_key = image_annotation_pairs[0]\n\nwords, boxes = get_ocr_words_boxes(test_image_path)\n\nprint(\"Number of OCR words:\", len(words))\nprint(\"First 20 words:\", words[:20])\nprint(\"First 5 boxes:\", boxes[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:10:58.938001Z","iopub.execute_input":"2026-02-23T06:10:58.938499Z","iopub.status.idle":"2026-02-23T06:11:02.589617Z","shell.execute_reply.started":"2026-02-23T06:10:58.938468Z","shell.execute_reply":"2026-02-23T06:11:02.588954Z"}},"outputs":[{"name":"stdout","text":"Number of OCR words: 377\nFirst 20 words: ['SUNSHINE', 'HOSPITALS', '(A', 'Unit', 'of', 'Sarvejana', 'Healthcare', 'Pvt.', 'Ltd.)', 'Laxmi', 'Sagar', 'Square,', 'Puri', '-', 'Cuttack', 'Road', 'Bhubaneswar,', 'Odisha', '-', '751006']\nFirst 5 boxes: [[409, 115, 483, 125], [488, 116, 570, 126], [383, 128, 394, 136], [398, 128, 418, 135], [421, 129, 432, 136]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ===== TEXT NORMALIZATION =====\n\ndef normalize_text(text):\n    if text is None:\n        return None\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9]', '', text)\n    return text\n\n\n# ===== ENTITY LIST =====\n\nschema_entities = [\n    \"HOSPITAL_NAME\",\n    \"PATIENT_NAME\",\n    \"BILL_NUMBER\",\n    \"BILL_DATE\",\n    \"ADMIT_DATE\",\n    \"DISCHARGE_DATE\",\n    \"MRN\",\n    \"TOTAL_AMOUNT\"\n]\n\n\n# ===== BIO LABEL GENERATION =====\n\ndef generate_bio_labels(words, annotation_dict):\n    labels = [\"O\"] * len(words)\n    norm_words = [normalize_text(w) for w in words]\n\n    for entity in schema_entities:\n        value = annotation_dict.get(entity.lower())\n        if value is None:\n            continue\n        \n        norm_value = normalize_text(value)\n        if not norm_value:\n            continue\n        \n        # sliding window search\n        for i in range(len(norm_words)):\n            combined = \"\"\n            for j in range(i, len(norm_words)):\n                combined += norm_words[j]\n                \n                if combined == norm_value:\n                    labels[i] = f\"B-{entity}\"\n                    for k in range(i+1, j+1):\n                        labels[k] = f\"I-{entity}\"\n                    break\n                \n                if len(combined) > len(norm_value):\n                    break\n    \n    return labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:07.667322Z","iopub.execute_input":"2026-02-23T06:11:07.668106Z","iopub.status.idle":"2026-02-23T06:11:07.677215Z","shell.execute_reply.started":"2026-02-23T06:11:07.668066Z","shell.execute_reply":"2026-02-23T06:11:07.676505Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Get first pair\ntest_image_path, test_key = image_annotation_pairs[0]\n\nannotation_data = annotations_raw[test_key]\n\nwords, boxes = get_ocr_words_boxes(test_image_path)\nbio_labels = generate_bio_labels(words, annotation_data)\n\n# Print matched tokens\nfor w, l in zip(words, bio_labels):\n    if l != \"O\":\n        print(w, \"->\", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:11.989596Z","iopub.execute_input":"2026-02-23T06:11:11.989889Z","iopub.status.idle":"2026-02-23T06:11:19.425733Z","shell.execute_reply.started":"2026-02-23T06:11:11.989863Z","shell.execute_reply":"2026-02-23T06:11:19.424924Z"}},"outputs":[{"name":"stdout","text":"SUNSHINE -> B-HOSPITAL_NAME\nHOSPITALS -> I-HOSPITAL_NAME\n03-Sep-2018 -> B-DISCHARGE_DATE\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"label_list = [\n    \"O\",\n\n    \"B-HOSPITAL_NAME\", \"I-HOSPITAL_NAME\",\n    \"B-BILL_NUMBER\", \"I-BILL_NUMBER\",\n    \"B-BILL_DATE\", \"I-BILL_DATE\",\n    \"B-ADMIT_DATE\", \"I-ADMIT_DATE\",\n    \"B-DISCHARGE_DATE\", \"I-DISCHARGE_DATE\",\n    \"B-MRN\", \"I-MRN\",\n    \"B-TOTAL_AMOUNT\", \"I-TOTAL_AMOUNT\"\n]\n\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:23.993014Z","iopub.execute_input":"2026-02-23T06:11:23.993327Z","iopub.status.idle":"2026-02-23T06:11:23.998491Z","shell.execute_reply.started":"2026-02-23T06:11:23.993298Z","shell.execute_reply":"2026-02-23T06:11:23.997578Z"}},"outputs":[{"name":"stdout","text":"{'O': 0, 'B-HOSPITAL_NAME': 1, 'I-HOSPITAL_NAME': 2, 'B-BILL_NUMBER': 3, 'I-BILL_NUMBER': 4, 'B-BILL_DATE': 5, 'I-BILL_DATE': 6, 'B-ADMIT_DATE': 7, 'I-ADMIT_DATE': 8, 'B-DISCHARGE_DATE': 9, 'I-DISCHARGE_DATE': 10, 'B-MRN': 11, 'I-MRN': 12, 'B-TOTAL_AMOUNT': 13, 'I-TOTAL_AMOUNT': 14}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n\nprocessor = LayoutLMv3Processor.from_pretrained(\n    \"microsoft/layoutlmv3-base\",\n    apply_ocr=False\n)\n\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\n    \"microsoft/layoutlmv3-base\",\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:27.472960Z","iopub.execute_input":"2026-02-23T06:11:27.473595Z","iopub.status.idle":"2026-02-23T06:11:29.325517Z","shell.execute_reply.started":"2026-02-23T06:11:27.473566Z","shell.execute_reply":"2026-02-23T06:11:29.323144Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/212 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e6aa72dc9945b3bfd69eef858faf09"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mLayoutLMv3ForTokenClassification LOAD REPORT\u001b[0m from: microsoft/layoutlmv3-base\nKey                                | Status     | \n-----------------------------------+------------+-\nlayoutlmv3.embeddings.position_ids | UNEXPECTED | \nclassifier.dense.weight            | MISSING    | \nclassifier.out_proj.weight         | MISSING    | \nclassifier.out_proj.bias           | MISSING    | \nclassifier.dense.bias              | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from PIL import Image\nimport os\n\n# ---- Pick first image ----\ntest_image_path = ordered_image_paths[0]\n\n# ---- Load image ----\nimage = Image.open(test_image_path).convert(\"RGB\")\nprint(\"Loaded image:\", test_image_path)\n\n# ---- Get index of image ----\nidx = ordered_image_paths.index(test_image_path)\n\n# ---- Get matching annotation key ----\nannotation_key = annotation_keys[idx]\n\n# ---- Load annotation ----\nannotation_data = annotations_raw[annotation_key]\n\n# ---- Run OCR ----\nwords, boxes = get_ocr_words_boxes(test_image_path)\n\n# ---- Generate BIO ----\nbio_labels = generate_bio_labels(words, annotation_data)\n\nprint(\"Annotation key used:\", annotation_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:33.222374Z","iopub.execute_input":"2026-02-23T06:11:33.223080Z","iopub.status.idle":"2026-02-23T06:11:40.981129Z","shell.execute_reply.started":"2026-02-23T06:11:33.223038Z","shell.execute_reply":"2026-02-23T06:11:40.980330Z"}},"outputs":[{"name":"stdout","text":"Loaded image: /kaggle/input/datasets/yamunaaab/artivatic-train-data/OneDrive_2026-02-22/Sample data/Detailed hospital bill/1981063-2.jpg\nAnnotation key used: image_1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"filename = os.path.basename(test_image_path)\n\n# Get index of this image in ordered_image_paths\nidx = ordered_image_paths.index(test_image_path)\n\n# Get corresponding annotation key\nannotation_key = annotation_keys[idx]\n\nannotation_data = annotations_raw[annotation_key]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:45.952125Z","iopub.execute_input":"2026-02-23T06:11:45.952448Z","iopub.status.idle":"2026-02-23T06:11:45.956595Z","shell.execute_reply.started":"2026-02-23T06:11:45.952419Z","shell.execute_reply":"2026-02-23T06:11:45.955846Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for w, l in zip(words, bio_labels):\n    if l != \"O\":\n        print(w, \"->\", l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:49.609018Z","iopub.execute_input":"2026-02-23T06:11:49.609866Z","iopub.status.idle":"2026-02-23T06:11:49.614804Z","shell.execute_reply.started":"2026-02-23T06:11:49.609823Z","shell.execute_reply":"2026-02-23T06:11:49.614018Z"}},"outputs":[{"name":"stdout","text":"SUNSHINE -> B-HOSPITAL_NAME\nHOSPITALS -> I-HOSPITAL_NAME\n03-Sep-2018 -> B-DISCHARGE_DATE\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ---- OCR ----\nwords, boxes = get_ocr_words_boxes(test_image_path)\n\n# ---- Map image to annotation using index (Option B) ----\nidx = ordered_image_paths.index(test_image_path)\nannotation_key = annotation_keys[idx]\nannotation_data = annotations_raw[annotation_key]\n\n# ---- Generate BIO ----\nbio_labels = generate_bio_labels(words, annotation_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:11:53.004762Z","iopub.execute_input":"2026-02-23T06:11:53.005418Z","iopub.status.idle":"2026-02-23T06:12:01.247988Z","shell.execute_reply.started":"2026-02-23T06:11:53.005389Z","shell.execute_reply":"2026-02-23T06:12:01.247220Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(\"Words:\", len(words))\nprint(\"Boxes:\", len(boxes))\nprint(\"BIO labels:\", len(bio_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:04.225660Z","iopub.execute_input":"2026-02-23T06:12:04.226377Z","iopub.status.idle":"2026-02-23T06:12:04.230551Z","shell.execute_reply.started":"2026-02-23T06:12:04.226340Z","shell.execute_reply":"2026-02-23T06:12:04.229837Z"}},"outputs":[{"name":"stdout","text":"Words: 377\nBoxes: 377\nBIO labels: 377\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"encoding = processor(\n    images=image,\n    text=words,        # <-- FIXED (not words=)\n    boxes=boxes,\n    word_labels=[label2id[label] for label in bio_labels],\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nfor k, v in encoding.items():\n    print(k, v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:08.093233Z","iopub.execute_input":"2026-02-23T06:12:08.094050Z","iopub.status.idle":"2026-02-23T06:12:08.180423Z","shell.execute_reply.started":"2026-02-23T06:12:08.094007Z","shell.execute_reply":"2026-02-23T06:12:08.179668Z"}},"outputs":[{"name":"stdout","text":"input_ids torch.Size([1, 512])\nattention_mask torch.Size([1, 512])\nbbox torch.Size([1, 512, 4])\nlabels torch.Size([1, 512])\npixel_values torch.Size([1, 3, 224, 224])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass HospitalDataset(Dataset):\n    def __init__(self, image_paths, annotations_raw, annotation_keys, processor, label2id):\n        self.image_paths = image_paths\n        self.annotations_raw = annotations_raw\n        self.annotation_keys = annotation_keys\n        self.processor = processor\n        self.label2id = label2id\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n\n        # Load image\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # OCR\n        words, boxes = get_ocr_words_boxes(image_path)\n\n        # Get matching annotation (Option B mapping)\n        annotation_key = self.annotation_keys[idx]\n        annotation_data = self.annotations_raw[annotation_key]\n\n        # Generate BIO labels\n        bio_labels = generate_bio_labels(words, annotation_data)\n\n        encoding = self.processor(\n            images=image,\n            text=words,\n            boxes=boxes,\n            word_labels=[self.label2id[label] for label in bio_labels],\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        # Remove batch dimension\n        item = {k: v.squeeze(0) for k, v in encoding.items()}\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:11.763681Z","iopub.execute_input":"2026-02-23T06:12:11.764282Z","iopub.status.idle":"2026-02-23T06:12:11.770879Z","shell.execute_reply.started":"2026-02-23T06:12:11.764239Z","shell.execute_reply":"2026-02-23T06:12:11.770202Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"dataset = HospitalDataset(\n    image_paths=ordered_image_paths,\n    annotations_raw=annotations_raw,\n    annotation_keys=annotation_keys,\n    processor=processor,\n    label2id=label2id\n)\n\nprint(\"Dataset size:\", len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:16.656318Z","iopub.execute_input":"2026-02-23T06:12:16.657132Z","iopub.status.idle":"2026-02-23T06:12:16.661834Z","shell.execute_reply.started":"2026-02-23T06:12:16.657090Z","shell.execute_reply":"2026-02-23T06:12:16.661173Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 30\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"sample = dataset[0]\n\nfor k, v in sample.items():\n    print(k, v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:20.137309Z","iopub.execute_input":"2026-02-23T06:12:20.138135Z","iopub.status.idle":"2026-02-23T06:12:27.688422Z","shell.execute_reply.started":"2026-02-23T06:12:20.138095Z","shell.execute_reply":"2026-02-23T06:12:27.687586Z"}},"outputs":[{"name":"stdout","text":"input_ids torch.Size([512])\nattention_mask torch.Size([512])\nbbox torch.Size([512, 4])\nlabels torch.Size([512])\npixel_values torch.Size([3, 224, 224])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n\nprint(\"Batches per epoch:\", len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:35.421402Z","iopub.execute_input":"2026-02-23T06:12:35.421757Z","iopub.status.idle":"2026-02-23T06:12:35.427220Z","shell.execute_reply.started":"2026-02-23T06:12:35.421728Z","shell.execute_reply":"2026-02-23T06:12:35.426018Z"}},"outputs":[{"name":"stdout","text":"Batches per epoch: 15\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:39.171608Z","iopub.execute_input":"2026-02-23T06:12:39.172324Z","iopub.status.idle":"2026-02-23T06:12:39.685170Z","shell.execute_reply.started":"2026-02-23T06:12:39.172265Z","shell.execute_reply":"2026-02-23T06:12:39.684484Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T06:12:49.222334Z","iopub.execute_input":"2026-02-23T06:12:49.223079Z","iopub.status.idle":"2026-02-23T06:12:49.228913Z","shell.execute_reply.started":"2026-02-23T06:12:49.223037Z","shell.execute_reply":"2026-02-23T06:12:49.228106Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import torch.nn as nn\n\n# Initialize all weights as 1\nclass_weights = torch.ones(len(label_list)).to(device)\n\n# Down-weight the O class heavily\nclass_weights[label2id[\"O\"]] = 0.05\n\nloss_fct = nn.CrossEntropyLoss(\n    weight=class_weights,\n    ignore_index=-100\n)\n\nprint(\"Class weights:\", class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:11:24.001622Z","iopub.execute_input":"2026-02-23T07:11:24.001982Z","iopub.status.idle":"2026-02-23T07:11:24.009487Z","shell.execute_reply.started":"2026-02-23T07:11:24.001952Z","shell.execute_reply":"2026-02-23T07:11:24.008799Z"}},"outputs":[{"name":"stdout","text":"Class weights: tensor([0.0500, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\n\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n    \n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            bbox=batch[\"bbox\"],\n            pixel_values=batch[\"pixel_values\"]\n        )\n    \n        logits = outputs.logits\n    \n        loss = loss_fct(\n            logits.view(-1, logits.shape[-1]),\n            batch[\"labels\"].view(-1)\n        )\n    \n        total_loss += loss.item()\n    \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n        loop.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"\\nEpoch {epoch+1} Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:12:17.546814Z","iopub.execute_input":"2026-02-23T07:12:17.547108Z","iopub.status.idle":"2026-02-23T07:20:58.245090Z","shell.execute_reply.started":"2026-02-23T07:12:17.547083Z","shell.execute_reply":"2026-02-23T07:20:58.244473Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:33<00:00, 10.24s/it, loss=0.804]  \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Average Loss: 0.0817\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:32<00:00,  6.14s/it, loss=0.0134]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Average Loss: 0.0902\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:31<00:00,  6.10s/it, loss=0.354]  \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Average Loss: 0.0763\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:31<00:00,  6.11s/it, loss=0.00887]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Average Loss: 0.2442\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:31<00:00,  6.11s/it, loss=0.00919]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Average Loss: 0.0833\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:22:23.408618Z","iopub.execute_input":"2026-02-23T07:22:23.408949Z","iopub.status.idle":"2026-02-23T07:22:23.416308Z","shell.execute_reply.started":"2026-02-23T07:22:23.408918Z","shell.execute_reply":"2026-02-23T07:22:23.415543Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"LayoutLMv3ForTokenClassification(\n  (layoutlmv3): LayoutLMv3Model(\n    (embeddings): LayoutLMv3TextEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (x_position_embeddings): Embedding(1024, 128)\n      (y_position_embeddings): Embedding(1024, 128)\n      (h_position_embeddings): Embedding(1024, 128)\n      (w_position_embeddings): Embedding(1024, 128)\n    )\n    (patch_embed): LayoutLMv3PatchEmbeddings(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (encoder): LayoutLMv3Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x LayoutLMv3Layer(\n          (attention): LayoutLMv3Attention(\n            (self): LayoutLMv3SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv3SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv3Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv3Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): LayoutLMv3ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=15, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"for i, label in enumerate(pred_labels):\n    if label != \"O\":\n        print(i, label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:22:32.665126Z","iopub.execute_input":"2026-02-23T07:22:32.665860Z","iopub.status.idle":"2026-02-23T07:22:32.669201Z","shell.execute_reply.started":"2026-02-23T07:22:32.665829Z","shell.execute_reply":"2026-02-23T07:22:32.668674Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"with torch.no_grad():\n    sample = dataset[0]\n\n    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n    bbox = sample[\"bbox\"].unsqueeze(0).to(device)\n    pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n\n    outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        bbox=bbox,\n        pixel_values=pixel_values\n    )\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:22:36.960868Z","iopub.execute_input":"2026-02-23T07:22:36.961645Z","iopub.status.idle":"2026-02-23T07:22:47.756064Z","shell.execute_reply.started":"2026-02-23T07:22:36.961618Z","shell.execute_reply":"2026-02-23T07:22:47.755255Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"pred_ids = predictions.squeeze(0).cpu().tolist()\npred_labels = [id2label[id] for id in pred_ids]\n\nprint(pred_labels[:50])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:22:52.035232Z","iopub.execute_input":"2026-02-23T07:22:52.035579Z","iopub.status.idle":"2026-02-23T07:22:52.040793Z","shell.execute_reply.started":"2026-02-23T07:22:52.035547Z","shell.execute_reply":"2026-02-23T07:22:52.040049Z"}},"outputs":[{"name":"stdout","text":"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"from transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\n\nprocessor = LayoutLMv3Processor.from_pretrained(\n    \"microsoft/layoutlmv3-base\",\n    apply_ocr=False\n)\n\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\n    \"microsoft/layoutlmv3-base\"\n)\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T07:48:06.574620Z","iopub.execute_input":"2026-02-23T07:48:06.575190Z","iopub.status.idle":"2026-02-23T07:48:08.349435Z","shell.execute_reply.started":"2026-02-23T07:48:06.575160Z","shell.execute_reply":"2026-02-23T07:48:08.348738Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/212 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9555a343d6a244718b52927f695c8b7d"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mLayoutLMv3ForQuestionAnswering LOAD REPORT\u001b[0m from: microsoft/layoutlmv3-base\nKey                                | Status     | \n-----------------------------------+------------+-\nlayoutlmv3.embeddings.position_ids | UNEXPECTED | \nqa_outputs.dense.bias              | MISSING    | \nqa_outputs.dense.weight            | MISSING    | \nqa_outputs.out_proj.weight         | MISSING    | \nqa_outputs.out_proj.bias           | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"LayoutLMv3ForQuestionAnswering(\n  (layoutlmv3): LayoutLMv3Model(\n    (embeddings): LayoutLMv3TextEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (x_position_embeddings): Embedding(1024, 128)\n      (y_position_embeddings): Embedding(1024, 128)\n      (h_position_embeddings): Embedding(1024, 128)\n      (w_position_embeddings): Embedding(1024, 128)\n    )\n    (patch_embed): LayoutLMv3PatchEmbeddings(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (encoder): LayoutLMv3Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x LayoutLMv3Layer(\n          (attention): LayoutLMv3Attention(\n            (self): LayoutLMv3SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv3SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv3Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv3Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n    )\n  )\n  (qa_outputs): LayoutLMv3ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"field_questions = {\n    \"hospital_name\": \"What is the hospital name?\",\n    \"bill_number\": \"What is the bill number?\",\n    \"bill_date\": \"What is the bill date?\",\n    \"admit_date\": \"What is the admit date?\",\n    \"discharge_date\": \"What is the discharge date?\",\n    \"mrn\": \"What is the MRN number?\",\n    \"total_amount\": \"What is the total amount?\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:49:02.195023Z","iopub.execute_input":"2026-02-23T08:49:02.195694Z","iopub.status.idle":"2026-02-23T08:49:02.199587Z","shell.execute_reply.started":"2026-02-23T08:49:02.195662Z","shell.execute_reply":"2026-02-23T08:49:02.198889Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"def normalize_token(t):\n    return \"\".join(e.lower() for e in t if e.isalnum())\n\ndef find_answer_span(words, answer_text):\n    if answer_text is None:\n        return None, None\n\n    words_norm = [normalize_token(w) for w in words]\n    answer_tokens = answer_text.split()\n    answer_norm = [normalize_token(w) for w in answer_tokens]\n\n    for i in range(len(words_norm) - len(answer_norm) + 1):\n        if words_norm[i:i+len(answer_norm)] == answer_norm:\n            return i, i + len(answer_norm) - 1\n\n    return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:49:04.592978Z","iopub.execute_input":"2026-02-23T08:49:04.593318Z","iopub.status.idle":"2026-02-23T08:49:04.599493Z","shell.execute_reply.started":"2026-02-23T08:49:04.593260Z","shell.execute_reply":"2026-02-23T08:49:04.598726Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"ANNOTATION_PATH = \"/kaggle/input/datasets/yamunaaab/final-json/json.txt\"\n\nwith open(ANNOTATION_PATH, \"r\") as f:\n    raw = f.read()\n\nlines = raw.split(\"\\n\")\n\nfixed_lines = []\nskip_next = False\n\nfor i in range(len(lines)):\n    if skip_next:\n        skip_next = False\n        continue\n\n    line = lines[i]\n\n    # Detect filename split across lines\n    if line.strip().startswith('\"') and line.strip().endswith('.jpg'):\n        # Merge with next non-empty line\n        next_line = lines[i+2]  # skip empty line in between\n        merged = line.strip() + '\": {'\n        fixed_lines.append(\"  \" + merged)\n        skip_next = True  # skip empty line\n    elif line.strip() == '\": {':\n        continue\n    else:\n        fixed_lines.append(line)\n\nfixed_raw = \"\\n\".join(fixed_lines)\n\n# Now try loading\nannotations_raw = json.loads(fixed_raw)\n\nprint(\"Loaded entries:\", len(annotations_raw))\nprint(list(annotations_raw.keys())[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:49:24.670849Z","iopub.execute_input":"2026-02-23T08:49:24.671610Z","iopub.status.idle":"2026-02-23T08:49:24.680676Z","shell.execute_reply.started":"2026-02-23T08:49:24.671578Z","shell.execute_reply":"2026-02-23T08:49:24.679915Z"}},"outputs":[{"name":"stdout","text":"Loaded entries: 29\n['1981063-2.jpg', '2042173-10.jpg', '2042485-05.jpg', '2048528-06.jpg', '2090022-final bill-1.jpg']\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"with open(\"/kaggle/working/clean_annotations.json\", \"w\") as f:\n    json.dump(annotations_raw, f, indent=2)\n\nprint(\"Clean JSON saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:49:28.962431Z","iopub.execute_input":"2026-02-23T08:49:28.962692Z","iopub.status.idle":"2026-02-23T08:49:28.967950Z","shell.execute_reply.started":"2026-02-23T08:49:28.962670Z","shell.execute_reply":"2026-02-23T08:49:28.967201Z"}},"outputs":[{"name":"stdout","text":"Clean JSON saved.\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport torch\n\nclass HospitalQADataset(Dataset):\n\n    def __init__(self, image_paths, annotations_raw, processor):\n        self.samples = []\n        self.processor = processor\n\n        for image_path in image_paths:\n            filename = os.path.basename(image_path)\n            annotation_data = annotations_raw.get(filename)\n\n            if annotation_data is None:\n                continue\n\n            words, boxes = get_ocr_words_boxes(image_path)\n\n            for field, question in field_questions.items():\n                value = annotation_data.get(field)\n\n                if value is None:\n                    continue\n\n                start_idx, end_idx = find_answer_span(words, value)\n\n                if start_idx is None:\n                    continue\n\n                self.samples.append({\n                    \"image_path\": image_path,\n                    \"question\": question,\n                    \"words\": words,\n                    \"boxes\": boxes,\n                    \"start_word\": start_idx,\n                    \"end_word\": end_idx\n                })\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n\n        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n\n        encoding = self.processor(\n            images=image,\n            text=sample[\"question\"],\n            text_pair=sample[\"words\"],\n            boxes=sample[\"boxes\"],\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        word_ids = encoding.word_ids(batch_index=0)\n\n        start_position = 0\n        end_position = 0\n\n        for token_idx, word_id in enumerate(word_ids):\n            if word_id == sample[\"start_word\"] and start_position == 0:\n                start_position = token_idx\n            if word_id == sample[\"end_word\"]:\n                end_position = token_idx\n\n        encoding[\"start_positions\"] = torch.tensor(start_position)\n        encoding[\"end_positions\"] = torch.tensor(end_position)\n\n        return {k: v.squeeze(0) if isinstance(v, torch.Tensor) else v\n                for k, v in encoding.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:53:30.548652Z","iopub.execute_input":"2026-02-23T08:53:30.548912Z","iopub.status.idle":"2026-02-23T08:53:30.558145Z","shell.execute_reply.started":"2026-02-23T08:53:30.548890Z","shell.execute_reply":"2026-02-23T08:53:30.557507Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"def __len__(self):\n    return len(self.samples)\n\ndef __getitem__(self, idx):\n    sample = self.samples[idx]\n\n    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n\n    encoding = self.processor(\n        images=image,\n        text=sample[\"question\"],\n        text_pair=sample[\"words\"],\n        boxes=sample[\"boxes\"],\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n\n    word_ids = encoding.word_ids(batch_index=0)\n\n    start_position = None\n    end_position = None\n\n    for token_idx, word_id in enumerate(word_ids):\n        if word_id == sample[\"start_word\"] and start_position is None:\n            start_position = token_idx\n        if word_id == sample[\"end_word\"]:\n            end_position = token_idx\n\n    if start_position is None:\n        start_position = 0\n    if end_position is None:\n        end_position = 0\n\n    encoding[\"start_positions\"] = torch.tensor(start_position)\n    encoding[\"end_positions\"] = torch.tensor(end_position)\n\n    return {k: v.squeeze(0) if isinstance(v, torch.Tensor) else v\n            for k, v in encoding.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:54:09.539182Z","iopub.execute_input":"2026-02-23T08:54:09.539755Z","iopub.status.idle":"2026-02-23T08:54:09.546986Z","shell.execute_reply.started":"2026-02-23T08:54:09.539723Z","shell.execute_reply":"2026-02-23T08:54:09.546332Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"qa_dataset = HospitalQADataset(\n    ordered_image_paths,\n    annotations_raw,\n    processor\n)\n\nprint(\"Total QA samples:\", len(qa_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:54:14.531580Z","iopub.execute_input":"2026-02-23T08:54:14.532310Z","iopub.status.idle":"2026-02-23T08:55:38.790717Z","shell.execute_reply.started":"2026-02-23T08:54:14.532281Z","shell.execute_reply":"2026-02-23T08:55:38.790090Z"}},"outputs":[{"name":"stdout","text":"Total QA samples: 76\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    qa_dataset,\n    batch_size=4,\n    shuffle=True\n)\n\nprint(\"Batches:\", len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:56:20.988827Z","iopub.execute_input":"2026-02-23T08:56:20.989435Z","iopub.status.idle":"2026-02-23T08:56:20.994404Z","shell.execute_reply.started":"2026-02-23T08:56:20.989406Z","shell.execute_reply":"2026-02-23T08:56:20.993578Z"}},"outputs":[{"name":"stdout","text":"Batches: 19\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"from transformers import LayoutLMv3ForQuestionAnswering\n\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(\n    \"microsoft/layoutlmv3-base\"\n)\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:56:42.458521Z","iopub.execute_input":"2026-02-23T08:56:42.458864Z","iopub.status.idle":"2026-02-23T08:56:43.209055Z","shell.execute_reply.started":"2026-02-23T08:56:42.458838Z","shell.execute_reply":"2026-02-23T08:56:43.208209Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/212 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa4d4a3bb472400a8f8eda97fe67b064"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mLayoutLMv3ForQuestionAnswering LOAD REPORT\u001b[0m from: microsoft/layoutlmv3-base\nKey                                | Status     | \n-----------------------------------+------------+-\nlayoutlmv3.embeddings.position_ids | UNEXPECTED | \nqa_outputs.dense.bias              | MISSING    | \nqa_outputs.dense.weight            | MISSING    | \nqa_outputs.out_proj.weight         | MISSING    | \nqa_outputs.out_proj.bias           | MISSING    | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n","output_type":"stream"},{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"LayoutLMv3ForQuestionAnswering(\n  (layoutlmv3): LayoutLMv3Model(\n    (embeddings): LayoutLMv3TextEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (x_position_embeddings): Embedding(1024, 128)\n      (y_position_embeddings): Embedding(1024, 128)\n      (h_position_embeddings): Embedding(1024, 128)\n      (w_position_embeddings): Embedding(1024, 128)\n    )\n    (patch_embed): LayoutLMv3PatchEmbeddings(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (encoder): LayoutLMv3Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x LayoutLMv3Layer(\n          (attention): LayoutLMv3Attention(\n            (self): LayoutLMv3SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv3SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv3Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv3Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n    )\n  )\n  (qa_outputs): LayoutLMv3ClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:56:53.844824Z","iopub.execute_input":"2026-02-23T08:56:53.845648Z","iopub.status.idle":"2026-02-23T08:56:53.851101Z","shell.execute_reply.started":"2026-02-23T08:56:53.845617Z","shell.execute_reply":"2026-02-23T08:56:53.850534Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"from tqdm import tqdm\n\nEPOCHS = 3\n\nmodel.train()\n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n\n    for batch in loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} Avg Loss:\", total_loss / len(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T08:57:07.612711Z","iopub.execute_input":"2026-02-23T08:57:07.613385Z","iopub.status.idle":"2026-02-23T08:58:01.669960Z","shell.execute_reply.started":"2026-02-23T08:57:07.613357Z","shell.execute_reply":"2026-02-23T08:58:01.669198Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:17<00:00,  1.08it/s, loss=4.11]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Avg Loss: 5.140728975597181\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:18<00:00,  1.05it/s, loss=3.11]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Avg Loss: 3.4224594768724943\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:18<00:00,  1.03it/s, loss=3.5] ","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Avg Loss: 2.7847067243174504\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"model.eval()\n\nraw_sample = qa_dataset.samples[0]\n\nimage = Image.open(raw_sample[\"image_path\"]).convert(\"RGB\")\n\nencoding = processor(\n    images=image,\n    text=raw_sample[\"question\"],\n    text_pair=raw_sample[\"words\"],\n    boxes=raw_sample[\"boxes\"],\n    padding=\"max_length\",\n    truncation=True,\n    return_tensors=\"pt\"\n)\n\nsequence_ids = encoding.sequence_ids(0)\n\nencoding = {k: v.to(device) for k, v in encoding.items()}\n\nwith torch.no_grad():\n    outputs = model(**encoding)\n\nstart_logits = outputs.start_logits.squeeze(0)\nend_logits = outputs.end_logits.squeeze(0)\n\n# Mask non-context tokens\nfor i, seq_id in enumerate(sequence_ids):\n    if seq_id != 1:\n        start_logits[i] = -1e9\n        end_logits[i] = -1e9\n\nstart_pred = torch.argmax(start_logits).item()\nend_pred = torch.argmax(end_logits).item()\n\ntokens = processor.tokenizer.convert_ids_to_tokens(\n    encoding[\"input_ids\"].squeeze(0)\n)\n\npredicted_tokens = tokens[start_pred:end_pred+1]\npredicted_text = processor.tokenizer.convert_tokens_to_string(predicted_tokens)\n\nprint(\"Question:\", raw_sample[\"question\"])\nprint(\"Predicted text:\", predicted_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:03:37.667298Z","iopub.execute_input":"2026-02-23T09:03:37.667978Z","iopub.status.idle":"2026-02-23T09:03:37.826904Z","shell.execute_reply.started":"2026-02-23T09:03:37.667946Z","shell.execute_reply":"2026-02-23T09:03:37.826101Z"}},"outputs":[{"name":"stdout","text":"Question: What is the hospital name?\nPredicted text:  SUNSHINE HOSPITALS\n","output_type":"stream"}],"execution_count":123},{"cell_type":"code","source":"input_ids = batch[\"input_ids\"].squeeze(0)\ntokens = processor.tokenizer.convert_ids_to_tokens(input_ids)\n\npredicted_tokens = tokens[start_pred:end_pred+1]\n\nprint(\"Predicted tokens:\", predicted_tokens)\n\npredicted_text = processor.tokenizer.convert_tokens_to_string(predicted_tokens)\n\nprint(\"Predicted text:\", predicted_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:04:13.527563Z","iopub.execute_input":"2026-02-23T09:04:13.528051Z","iopub.status.idle":"2026-02-23T09:04:13.541461Z","shell.execute_reply.started":"2026-02-23T09:04:13.528023Z","shell.execute_reply":"2026-02-23T09:04:13.540655Z"}},"outputs":[{"name":"stdout","text":"Predicted tokens: ['Ä SUN', 'SH', 'INE', 'Ä H', 'OSP', 'IT', 'ALS']\nPredicted text:  SUNSHINE HOSPITALS\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/layoutlmv3_qa_model\")\nprocessor.save_pretrained(\"/kaggle/working/layoutlmv3_qa_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:07:00.729590Z","iopub.execute_input":"2026-02-23T09:07:00.730247Z","iopub.status.idle":"2026-02-23T09:07:01.800515Z","shell.execute_reply.started":"2026-02-23T09:07:00.730216Z","shell.execute_reply":"2026-02-23T09:07:01.799887Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f6a69f9aad46fdb9994fd9962edc4f"}},"metadata":{}},{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/layoutlmv3_qa_model/processor_config.json']"},"metadata":{}}],"execution_count":125},{"cell_type":"code","source":"def extract_field(image_path, field_name):\n    question = field_questions[field_name]\n\n    words, boxes = get_ocr_words_boxes(image_path)\n\n    image = Image.open(image_path).convert(\"RGB\")\n\n    encoding = processor(\n        images=image,\n        text=question,\n        text_pair=words,\n        boxes=boxes,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n\n    sequence_ids = encoding.sequence_ids(0)\n    encoding = {k: v.to(device) for k, v in encoding.items()}\n\n    with torch.no_grad():\n        outputs = model(**encoding)\n\n    start_logits = outputs.start_logits.squeeze(0)\n    end_logits = outputs.end_logits.squeeze(0)\n\n    for i, seq_id in enumerate(sequence_ids):\n        if seq_id != 1:\n            start_logits[i] = -1e9\n            end_logits[i] = -1e9\n\n    start_pred = torch.argmax(start_logits).item()\n    end_pred = torch.argmax(end_logits).item()\n\n    tokens = processor.tokenizer.convert_ids_to_tokens(\n        encoding[\"input_ids\"].squeeze(0)\n    )\n\n    predicted_tokens = tokens[start_pred:end_pred+1]\n    return processor.tokenizer.convert_tokens_to_string(predicted_tokens).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:08:23.998415Z","iopub.execute_input":"2026-02-23T09:08:23.999152Z","iopub.status.idle":"2026-02-23T09:08:24.005201Z","shell.execute_reply.started":"2026-02-23T09:08:23.999123Z","shell.execute_reply":"2026-02-23T09:08:24.004640Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"import re\n\ndef normalize_text(s):\n    s = s.lower()\n    s = re.sub(r'[^a-z0-9]', '', s)\n    return s\n\ndef compute_f1(pred, truth):\n    pred_tokens = normalize_text(pred)\n    truth_tokens = normalize_text(truth)\n\n    if pred_tokens == truth_tokens:\n        return 1.0\n\n    common = set(pred_tokens) & set(truth_tokens)\n    if len(common) == 0:\n        return 0.0\n\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(truth_tokens)\n    return 2 * (precision * recall) / (precision + recall)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:09:47.997246Z","iopub.execute_input":"2026-02-23T09:09:47.997905Z","iopub.status.idle":"2026-02-23T09:09:48.003066Z","shell.execute_reply.started":"2026-02-23T09:09:47.997875Z","shell.execute_reply":"2026-02-23T09:09:48.002408Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"model.eval()\n\nexact_matches = 0\ntotal = 0\nf1_total = 0\n\nfor raw_sample in qa_dataset.samples:\n\n    image_path = raw_sample[\"image_path\"]\n    question = raw_sample[\"question\"]\n    true_answer = \" \".join(\n        raw_sample[\"words\"][raw_sample[\"start_word\"]:raw_sample[\"end_word\"]+1]\n    )\n\n    predicted = extract_field(image_path, \n                              [k for k,v in field_questions.items() \n                               if v == question][0])\n\n    total += 1\n\n    if normalize_text(predicted) == normalize_text(true_answer):\n        exact_matches += 1\n\n    f1_total += compute_f1(predicted, true_answer)\n\nexact_match_score = exact_matches / total\navg_f1 = f1_total / total\n\nprint(\"Total samples:\", total)\nprint(\"Exact Match:\", exact_match_score)\nprint(\"Average F1:\", avg_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T09:10:03.913322Z","iopub.execute_input":"2026-02-23T09:10:03.914072Z","iopub.status.idle":"2026-02-23T09:12:46.808502Z","shell.execute_reply.started":"2026-02-23T09:10:03.914024Z","shell.execute_reply":"2026-02-23T09:12:46.807804Z"}},"outputs":[{"name":"stdout","text":"Total samples: 76\nExact Match: 0.21052631578947367\nAverage F1: 0.3363177029845176\n","output_type":"stream"}],"execution_count":129}]}